# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

input {
#  beats {
#    port => 5044
#  }
   kafka{
      enable_auto_commit => true
      #auto_commit_interval_ms => "1000"
      codec => "json"
      bootstrap_servers => ["localhost:9092"]
      #client_id => "shence"
      #auto_offset_reset => "latest"
      consumer_threads => 5
      #decorate_events  => true
      topics => ["logcentre"]
     # type => "bhy"
  }
}
filter {
    grok {
        match => {
            "message" => '(?<clientip>[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}) - - \[(?<requesttime>[^ ]+ \+[0-9]+)\] "(?<requesttype>[A-Z]+) (?<requesturl>[^ ]+) HTTP/\d.\d" (?<status>[0-9]+) (?<bodysize>[0-9]+) "[^"]+" "(?<ua>[^"]+)"'
        }
        remove_field => ["message","@version","path"]
    }
date {
        match => ["requesttime", "dd/MMM/yyyy:HH:mm:ss Z"]
        target => "@timestamp"
    }
}
output {
  stdout{ codec=>rubydebug}
  if [fields][logtype] == "syslog"{
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
    #user => "elastic"
    #password => "changeme"
  }
  }
#	jdbc {
#        driver_class => "com.mysql.jdbc.Driver"
#		connection_string => "jdbc:mysql://localhost/loggater?user=root&password=Suibin@520"
#		statement => [ "INSERT INTO log (host, timestamp, message) VALUES(?, CAST(? AS timestamp), ?)", "host", "@timestamp", "message" ]
#	}
}
